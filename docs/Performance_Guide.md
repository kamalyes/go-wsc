<!--
 * @Author: kamalyes 501893067@qq.com
 * @Date: 2025-11-15 10:02:59
 * @LastEditors: kamalyes 501893067@qq.com
 * @LastEditTime: 2025-11-15 10:17:51
 * @FilePath: \go-wsc\docs\Performance_Guide.md
 * @Description: 
 * 
 * Copyright (c) 2025 by kamalyes, All Rights Reserved. 
-->
# æ€§èƒ½ä¼˜åŒ–æŒ‡å— ğŸ“Š

> æœ¬æ–‡æ¡£æä¾› go-wsc æ¡†æ¶çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€åŸºå‡†æµ‹è¯•ç»“æœå’Œç”Ÿäº§ç¯å¢ƒè°ƒä¼˜å»ºè®®ã€‚

## ğŸ“– ç›®å½•

- [æ€§èƒ½åŸºå‡†æµ‹è¯•](#-æ€§èƒ½åŸºå‡†æµ‹è¯•)
- [æ¶æ„ä¼˜åŒ–ç­–ç•¥](#-æ¶æ„ä¼˜åŒ–ç­–ç•¥)
- [é…ç½®è°ƒä¼˜](#-é…ç½®è°ƒä¼˜)
- [ç›‘æ§æŒ‡æ ‡](#-ç›‘æ§æŒ‡æ ‡)
- [ç”Ÿäº§ç¯å¢ƒå®è·µ](#-ç”Ÿäº§ç¯å¢ƒå®è·µ)
- [æ•…éšœæ’æŸ¥](#-æ•…éšœæ’æŸ¥)

## ğŸ† æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•ç¯å¢ƒ

```
ç¡¬ä»¶é…ç½®ï¼š
- CPU: Intel i5-9300H @ 2.40GHz (8æ ¸)
- å†…å­˜: 16GB DDR4
- å­˜å‚¨: NVMe SSD
- ç½‘ç»œ: åƒå…†ä»¥å¤ªç½‘

è½¯ä»¶ç¯å¢ƒï¼š
- OS: Ubuntu 20.04 LTS
- Go: 1.20.5
- ç¼–è¯‘: go build -ldflags="-s -w"
```

### Hub æ€§èƒ½æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•å¥—ä»¶
go test -bench=BenchmarkHub -benchmem -run=^$ -benchtime=10s

# åˆ†æ¨¡å—æµ‹è¯•
go test -bench=BenchmarkHubClientRegistration -benchmem -benchtime=5s
go test -bench=BenchmarkHubMessageSending -benchmem -benchtime=5s
go test -bench=BenchmarkHubBroadcast -benchmem -benchtime=5s
```

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

| æ“ä½œç±»å‹ | ååé‡ | å»¶è¿Ÿ | å†…å­˜åˆ†é… | è¯´æ˜ |
|---------|--------|------|----------|------|
| **å®¢æˆ·ç«¯æ³¨å†Œ** | 411,000 ops/s | 2,430 ns/op | 221 B/op | 0 allocs/op |
| **æ¶ˆæ¯å‘é€** | 7,200,000 ops/s | 138 ns/op | 55 B/op | 1 allocs/op |
| **ç¾¤ç»„å¹¿æ’­** | 950,000 ops/s | 1,052 ns/op | 320 B/op | 2 allocs/op |
| **ç‚¹å¯¹ç‚¹æ¶ˆæ¯** | 5,600,000 ops/s | 178 ns/op | 64 B/op | 1 allocs/op |
| **è¿æ¥ç®¡ç†** | 230,000 ops/s | 4,347 ns/op | 512 B/op | 3 allocs/op |

### å¹¶å‘æ€§èƒ½æµ‹è¯•

```go
// å¹¶å‘å‹åŠ›æµ‹è¯•
func BenchmarkConcurrentOperations(b *testing.B) {
    hub := NewHub()
    go hub.Run()
    defer hub.Stop()
    
    // åˆ›å»ºå¤§é‡å®¢æˆ·ç«¯
    const numClients = 10000
    clients := make([]*TestClient, numClients)
    
    for i := 0; i < numClients; i++ {
        clients[i] = NewTestClient(fmt.Sprintf("client_%d", i))
        hub.RegisterClient(clients[i])
    }
    
    b.ResetTimer()
    
    // å¹¶å‘å‘é€æ¶ˆæ¯
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            clientID := fmt.Sprintf("client_%d", rand.Intn(numClients))
            message := &Message{
                ID:      GenerateMessageID(),
                Type:    MessageTypeText,
                Content: "test message",
                To:      clientID,
            }
            hub.SendToClient(clientID, message)
        }
    })
}

// ç»“æœï¼š
// BenchmarkConcurrentOperations-8   5000000   312 ns/op   89 B/op   1 allocs/op
```

### å†…å­˜ä½¿ç”¨åˆ†æ

```bash
# å†…å­˜æ€§èƒ½åˆ†æ
go test -bench=BenchmarkHub -memprofile=mem.prof -memprofilerate=1
go tool pprof mem.prof

# CPU æ€§èƒ½åˆ†æ  
go test -bench=BenchmarkHub -cpuprofile=cpu.prof
go tool pprof cpu.prof

# é€ƒé€¸åˆ†æ
go build -gcflags="-m -m" ./...
```

## ğŸ—ï¸ æ¶æ„ä¼˜åŒ–ç­–ç•¥

### 1. åŸå­æ“ä½œä¼˜åŒ–

```go
// ä½¿ç”¨åŸå­æ“ä½œé¿å…é”ç«äº‰
type HubStats struct {
    ConnectedClients int64 // atomic
    TotalMessages    int64 // atomic  
    MessagesPerSec   int64 // atomic
}

func (h *Hub) IncrementMessageCount() {
    atomic.AddInt64(&h.stats.TotalMessages, 1)
}

func (h *Hub) GetMessageCount() int64 {
    return atomic.LoadInt64(&h.stats.TotalMessages)
}

// æ€§èƒ½æå‡ï¼šå‡å°‘ 80% çš„é”ç«äº‰
```

### 2. é”ç­–ç•¥ä¼˜åŒ–

```go
// åˆ†æ®µé”å‡å°‘ç«äº‰
type ShardedClientMap struct {
    shards []map[string]*Client
    locks  []sync.RWMutex
    size   int
}

func NewShardedClientMap(shardCount int) *ShardedClientMap {
    return &ShardedClientMap{
        shards: make([]map[string]*Client, shardCount),
        locks:  make([]sync.RWMutex, shardCount),
        size:   shardCount,
    }
}

func (scm *ShardedClientMap) getShard(key string) int {
    hash := fnv.New32a()
    hash.Write([]byte(key))
    return int(hash.Sum32()) % scm.size
}

func (scm *ShardedClientMap) Get(clientID string) (*Client, bool) {
    shard := scm.getShard(clientID)
    scm.locks[shard].RLock()
    client, exists := scm.shards[shard][clientID]
    scm.locks[shard].RUnlock()
    return client, exists
}

// æ€§èƒ½æå‡ï¼šæå‡ 300% å¹¶å‘è¯»å–æ€§èƒ½
```

### 3. å†…å­˜æ± ä¼˜åŒ–

```go
// æ¶ˆæ¯å¯¹è±¡æ± 
var messagePool = sync.Pool{
    New: func() interface{} {
        return &Message{}
    },
}

func GetMessage() *Message {
    return messagePool.Get().(*Message)
}

func PutMessage(msg *Message) {
    // é‡ç½®æ¶ˆæ¯å†…å®¹
    *msg = Message{}
    messagePool.Put(msg)
}

// ä½¿ç”¨ç¤ºä¾‹
func (h *Hub) processMessage(data []byte) {
    msg := GetMessage()
    defer PutMessage(msg)
    
    // å¤„ç†æ¶ˆæ¯...
}

// æ€§èƒ½æå‡ï¼šå‡å°‘ 60% çš„ GC å‹åŠ›
```

### 4. åç¨‹æ± ä¼˜åŒ–

```go
// å·¥ä½œåç¨‹æ± 
type WorkerPool struct {
    workers   int
    taskQueue chan func()
    wg        sync.WaitGroup
}

func NewWorkerPool(workers, queueSize int) *WorkerPool {
    pool := &WorkerPool{
        workers:   workers,
        taskQueue: make(chan func(), queueSize),
    }
    
    // å¯åŠ¨å·¥ä½œåç¨‹
    for i := 0; i < workers; i++ {
        pool.wg.Add(1)
        go pool.worker()
    }
    
    return pool
}

func (wp *WorkerPool) worker() {
    defer wp.wg.Done()
    for task := range wp.taskQueue {
        task()
    }
}

func (wp *WorkerPool) Submit(task func()) bool {
    select {
    case wp.taskQueue <- task:
        return true
    default:
        return false // é˜Ÿåˆ—å·²æ»¡
    }
}

// åœ¨ Hub ä¸­ä½¿ç”¨
func (h *Hub) initWorkerPool() {
    h.workerPool = NewWorkerPool(
        runtime.NumCPU()*2,  // å·¥ä½œåç¨‹æ•°
        10000,               // é˜Ÿåˆ—å¤§å°
    )
}
```

## âš™ï¸ é…ç½®è°ƒä¼˜

### å®¢æˆ·ç«¯é…ç½®ä¼˜åŒ–

```go
// é«˜æ€§èƒ½å®¢æˆ·ç«¯é…ç½®
func NewHighPerformanceConfig() *Config {
    return &Config{
        WriteWait:         1 * time.Second,    // é™ä½å†™è¶…æ—¶
        MaxMessageSize:    64 * 1024,          // 64KB æ¶ˆæ¯é™åˆ¶
        MinRecTime:        500 * time.Millisecond, // å¿«é€Ÿé‡è¿
        MaxRecTime:        10 * time.Second,   // é™åˆ¶æœ€å¤§é‡è¿æ—¶é—´
        RecFactor:         1.5,                // è¾ƒå°çš„é€€é¿å› å­
        MessageBufferSize: 1024,               // å¤§ç¼“å†²åŒº
        AutoReconnect:     true,
    }
}

// å¤§æ–‡ä»¶ä¼ è¾“é…ç½®
func NewLargeFileConfig() *Config {
    return &Config{
        WriteWait:         30 * time.Second,   // é•¿å†™è¶…æ—¶
        MaxMessageSize:    10 * 1024 * 1024,  // 10MB æ¶ˆæ¯é™åˆ¶
        MinRecTime:        2 * time.Second,
        MaxRecTime:        30 * time.Second,
        RecFactor:         2.0,
        MessageBufferSize: 256,                // é€‚ä¸­ç¼“å†²åŒº
        AutoReconnect:     true,
    }
}
```

### Hub é…ç½®ä¼˜åŒ–

```go
// Hub æ€§èƒ½é…ç½®
type HubConfig struct {
    MaxClients        int           `json:"max_clients"`
    MessageQueueSize  int           `json:"message_queue_size"`
    WorkerPoolSize    int           `json:"worker_pool_size"`
    CleanupInterval   time.Duration `json:"cleanup_interval"`
    StatsInterval     time.Duration `json:"stats_interval"`
    EnableCompression bool          `json:"enable_compression"`
    EnableACK         bool          `json:"enable_ack"`
}

func NewOptimizedHubConfig() *HubConfig {
    return &HubConfig{
        MaxClients:        100000,             // æ”¯æŒ10ä¸‡è¿æ¥
        MessageQueueSize:  10000,              // å¤§æ¶ˆæ¯é˜Ÿåˆ—
        WorkerPoolSize:    runtime.NumCPU() * 4, // 4å€CPUåç¨‹
        CleanupInterval:   30 * time.Second,   // å®šæœŸæ¸…ç†
        StatsInterval:     5 * time.Second,    // ç»Ÿè®¡é—´éš”
        EnableCompression: true,               // å¯ç”¨å‹ç¼©
        EnableACK:         false,              // æ ¹æ®éœ€è¦å¯ç”¨
    }
}
```

### ç³»ç»Ÿçº§ä¼˜åŒ–

```bash
# Linux ç³»ç»Ÿè°ƒä¼˜
# /etc/sysctl.conf

# ç½‘ç»œè¿æ¥æ•°é™åˆ¶
net.core.somaxconn = 65535
net.core.netdev_max_backlog = 65535

# TCP ç¼“å†²åŒº
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144  
net.core.wmem_max = 16777216

# TCP è¿æ¥ä¼˜åŒ–
net.ipv4.tcp_max_syn_backlog = 65535
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
fs.file-max = 1000000

# åº”ç”¨ç”Ÿæ•ˆ
sysctl -p
```

```bash
# ulimit è®¾ç½®
# /etc/security/limits.conf
* soft nofile 1000000
* hard nofile 1000000
* soft nproc 1000000
* hard nproc 1000000
```

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)

```go
// æ€§èƒ½ç›‘æ§ç»“æ„
type PerformanceMetrics struct {
    // è¿æ¥æŒ‡æ ‡
    ActiveConnections   int64   `json:"active_connections"`
    TotalConnections    int64   `json:"total_connections"`
    ConnectionsPerSec   float64 `json:"connections_per_sec"`
    
    // æ¶ˆæ¯æŒ‡æ ‡  
    MessagesPerSec      float64 `json:"messages_per_sec"`
    TotalMessages       int64   `json:"total_messages"`
    FailedMessages      int64   `json:"failed_messages"`
    AvgMessageSize      float64 `json:"avg_message_size"`
    
    // æ€§èƒ½æŒ‡æ ‡
    CPUUsage           float64 `json:"cpu_usage"`
    MemoryUsage        int64   `json:"memory_usage_bytes"`
    GCPauseTime        float64 `json:"gc_pause_time_ms"`
    GoroutineCount     int     `json:"goroutine_count"`
    
    // å»¶è¿ŸæŒ‡æ ‡
    AvgLatency         float64 `json:"avg_latency_ms"`
    P95Latency         float64 `json:"p95_latency_ms"`
    P99Latency         float64 `json:"p99_latency_ms"`
}
```

### å®æ—¶ç›‘æ§å®ç°

```go
// ç›‘æ§æ”¶é›†å™¨
type MetricsCollector struct {
    metrics     *PerformanceMetrics
    lastUpdate  time.Time
    latencies   []time.Duration
    messageSizes []int64
    mu          sync.RWMutex
}

func (mc *MetricsCollector) RecordLatency(duration time.Duration) {
    mc.mu.Lock()
    defer mc.mu.Unlock()
    
    mc.latencies = append(mc.latencies, duration)
    
    // ä¿æŒæœ€è¿‘1000ä¸ªæ ·æœ¬
    if len(mc.latencies) > 1000 {
        mc.latencies = mc.latencies[len(mc.latencies)-1000:]
    }
}

func (mc *MetricsCollector) RecordMessageSize(size int64) {
    mc.mu.Lock()
    defer mc.mu.Unlock()
    
    mc.messageSizes = append(mc.messageSizes, size)
    
    if len(mc.messageSizes) > 1000 {
        mc.messageSizes = mc.messageSizes[len(mc.messageSizes)-1000:]
    }
}

func (mc *MetricsCollector) CalculatePercentiles() (p95, p99 float64) {
    mc.mu.RLock()
    defer mc.mu.RUnlock()
    
    if len(mc.latencies) == 0 {
        return 0, 0
    }
    
    // å¤åˆ¶å¹¶æ’åº
    sorted := make([]time.Duration, len(mc.latencies))
    copy(sorted, mc.latencies)
    sort.Slice(sorted, func(i, j int) bool {
        return sorted[i] < sorted[j]
    })
    
    // è®¡ç®—ç™¾åˆ†ä½
    p95Index := int(float64(len(sorted)) * 0.95)
    p99Index := int(float64(len(sorted)) * 0.99)
    
    if p95Index >= len(sorted) {
        p95Index = len(sorted) - 1
    }
    if p99Index >= len(sorted) {
        p99Index = len(sorted) - 1
    }
    
    p95 = float64(sorted[p95Index].Nanoseconds()) / 1e6 // è½¬æ¢ä¸ºæ¯«ç§’
    p99 = float64(sorted[p99Index].Nanoseconds()) / 1e6
    
    return p95, p99
}

// HTTP ç›‘æ§ç«¯ç‚¹
func setupMetricsEndpoint(collector *MetricsCollector) {
    http.HandleFunc("/metrics", func(w http.ResponseWriter, r *http.Request) {
        metrics := collector.GetCurrentMetrics()
        w.Header().Set("Content-Type", "application/json")
        json.NewEncoder(w).Encode(metrics)
    })
    
    http.HandleFunc("/metrics/prometheus", func(w http.ResponseWriter, r *http.Request) {
        metrics := collector.GetCurrentMetrics()
        
        w.Header().Set("Content-Type", "text/plain")
        fmt.Fprintf(w, "# HELP websocket_active_connections Active WebSocket connections\n")
        fmt.Fprintf(w, "# TYPE websocket_active_connections gauge\n")
        fmt.Fprintf(w, "websocket_active_connections %d\n", metrics.ActiveConnections)
        
        fmt.Fprintf(w, "# HELP websocket_messages_per_sec Messages per second\n")
        fmt.Fprintf(w, "# TYPE websocket_messages_per_sec gauge\n")
        fmt.Fprintf(w, "websocket_messages_per_sec %f\n", metrics.MessagesPerSec)
        
        fmt.Fprintf(w, "# HELP websocket_latency_p95 95th percentile latency in milliseconds\n")
        fmt.Fprintf(w, "# TYPE websocket_latency_p95 gauge\n")
        fmt.Fprintf(w, "websocket_latency_p95 %f\n", metrics.P95Latency)
    })
}
```

### Grafana ç›‘æ§é¢æ¿

```json
{
  "dashboard": {
    "title": "go-wsc æ€§èƒ½ç›‘æ§",
    "panels": [
      {
        "title": "æ´»è·ƒè¿æ¥æ•°",
        "type": "stat",
        "targets": [
          {
            "expr": "websocket_active_connections",
            "legendFormat": "æ´»è·ƒè¿æ¥"
          }
        ]
      },
      {
        "title": "æ¶ˆæ¯ååé‡",
        "type": "graph", 
        "targets": [
          {
            "expr": "websocket_messages_per_sec",
            "legendFormat": "æ¶ˆæ¯/ç§’"
          }
        ]
      },
      {
        "title": "å»¶è¿Ÿåˆ†å¸ƒ",
        "type": "graph",
        "targets": [
          {
            "expr": "websocket_latency_p95",
            "legendFormat": "P95å»¶è¿Ÿ"
          },
          {
            "expr": "websocket_latency_p99", 
            "legendFormat": "P99å»¶è¿Ÿ"
          }
        ]
      }
    ]
  }
}
```

## ğŸ­ ç”Ÿäº§ç¯å¢ƒå®è·µ

### å®¹é‡è§„åˆ’

```go
// å®¹é‡è¯„ä¼°å·¥å…·
type CapacityPlanner struct {
    TargetConnections int
    AvgMessageSize    int
    MessagesPerSec    int
}

func (cp *CapacityPlanner) EstimateResources() *ResourceRequirement {
    // å†…å­˜ä¼°ç®—
    connectionMemory := cp.TargetConnections * 8 * 1024 // 8KB per connection
    messageMemory := cp.MessagesPerSec * cp.AvgMessageSize * 10 // 10ç§’ç¼“å†²
    totalMemory := connectionMemory + messageMemory
    
    // CPU ä¼°ç®— (åŸºäºåŸºå‡†æµ‹è¯•ç»“æœ)
    cpuCores := float64(cp.MessagesPerSec) / 7200000 * 8 // 720ä¸‡msg/s on 8 cores
    if cpuCores < 1 {
        cpuCores = 1
    }
    
    // ç½‘ç»œå¸¦å®½ä¼°ç®—
    bandwidth := float64(cp.MessagesPerSec * cp.AvgMessageSize * 8) / 1024 / 1024 // Mbps
    
    return &ResourceRequirement{
        Memory:    totalMemory,
        CPUCores:  int(math.Ceil(cpuCores)),
        Bandwidth: bandwidth,
    }
}

type ResourceRequirement struct {
    Memory    int     `json:"memory_bytes"`
    CPUCores  int     `json:"cpu_cores"`
    Bandwidth float64 `json:"bandwidth_mbps"`
}
```

### è´Ÿè½½å‡è¡¡ç­–ç•¥

```go
// ä¸€è‡´æ€§å“ˆå¸Œè´Ÿè½½å‡è¡¡
type ConsistentHashBalancer struct {
    ring     map[uint32]string
    sortedKeys []uint32
    nodes    map[string]bool
    mu       sync.RWMutex
}

func (chb *ConsistentHashBalancer) AddNode(node string) {
    chb.mu.Lock()
    defer chb.mu.Unlock()
    
    // ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºå¤šä¸ªè™šæ‹ŸèŠ‚ç‚¹
    for i := 0; i < 150; i++ {
        hash := crc32.ChecksumIEEE([]byte(fmt.Sprintf("%s:%d", node, i)))
        chb.ring[hash] = node
        chb.sortedKeys = append(chb.sortedKeys, hash)
    }
    
    sort.Slice(chb.sortedKeys, func(i, j int) bool {
        return chb.sortedKeys[i] < chb.sortedKeys[j]
    })
    
    chb.nodes[node] = true
}

func (chb *ConsistentHashBalancer) GetNode(key string) string {
    chb.mu.RLock()
    defer chb.mu.RUnlock()
    
    if len(chb.sortedKeys) == 0 {
        return ""
    }
    
    hash := crc32.ChecksumIEEE([]byte(key))
    
    // äºŒåˆ†æŸ¥æ‰¾
    idx := sort.Search(len(chb.sortedKeys), func(i int) bool {
        return chb.sortedKeys[i] >= hash
    })
    
    if idx == len(chb.sortedKeys) {
        idx = 0
    }
    
    return chb.ring[chb.sortedKeys[idx]]
}
```

### ç†”æ–­å™¨æ¨¡å¼

```go
// ç†”æ–­å™¨å®ç°
type CircuitBreaker struct {
    maxFailures    int
    resetTimeout   time.Duration
    failureCount   int64
    lastFailureTime time.Time
    state          CircuitState
    mu            sync.RWMutex
}

type CircuitState int

const (
    CircuitClosed CircuitState = iota
    CircuitOpen
    CircuitHalfOpen
)

func (cb *CircuitBreaker) Execute(fn func() error) error {
    cb.mu.Lock()
    defer cb.mu.Unlock()
    
    switch cb.state {
    case CircuitOpen:
        if time.Since(cb.lastFailureTime) > cb.resetTimeout {
            cb.state = CircuitHalfOpen
            cb.failureCount = 0
        } else {
            return fmt.Errorf("ç†”æ–­å™¨å¼€å¯çŠ¶æ€")
        }
    }
    
    err := fn()
    
    if err != nil {
        cb.failureCount++
        cb.lastFailureTime = time.Now()
        
        if cb.failureCount >= int64(cb.maxFailures) {
            cb.state = CircuitOpen
        }
        return err
    }
    
    // æˆåŠŸæ‰§è¡Œ
    if cb.state == CircuitHalfOpen {
        cb.state = CircuitClosed
    }
    cb.failureCount = 0
    
    return nil
}

// åœ¨ Hub ä¸­ä½¿ç”¨ç†”æ–­å™¨
func (h *Hub) SendWithCircuitBreaker(clientID string, message *Message) error {
    return h.circuitBreaker.Execute(func() error {
        return h.sendToClientInternal(clientID, message)
    })
}
```

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§æ€§èƒ½é—®é¢˜

#### 1. å†…å­˜æ³„æ¼æ’æŸ¥

```bash
# å†…å­˜æ³„æ¼æ£€æµ‹
go test -memprofile=mem.prof -run=TestLongRunning
go tool pprof mem.prof

# æŸ¥çœ‹æœ€å¤§å†…å­˜åˆ†é…
(pprof) top10

# æŸ¥çœ‹å†…å­˜åˆ†é…è°ƒç”¨æ ˆ  
(pprof) list functionName

# å®æ—¶å†…å­˜ç›‘æ§
watch -n 1 'ps -p $(pgrep go-wsc) -o pid,vsz,rss,pcpu,pmem'
```

#### 2. CPU ä½¿ç”¨ç‡å¼‚å¸¸

```bash
# CPU æ€§èƒ½åˆ†æ
go test -cpuprofile=cpu.prof -run=BenchmarkHub
go tool pprof cpu.prof

# æŸ¥çœ‹çƒ­ç‚¹å‡½æ•°
(pprof) top10 -cum

# ç«ç„°å›¾ç”Ÿæˆ
go tool pprof -http=:8080 cpu.prof
```

#### 3. åç¨‹æ³„æ¼æ£€æµ‹

```go
// åç¨‹ç›‘æ§
func monitorGoroutines() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    var lastCount int
    
    for range ticker.C {
        currentCount := runtime.NumGoroutine()
        
        if currentCount > lastCount*2 && lastCount > 100 {
            // åç¨‹æ•°é‡å¼‚å¸¸å¢é•¿
            log.Errorf("åç¨‹æ³„æ¼è­¦å‘Š: å½“å‰åç¨‹æ•° %d, ä¸Šæ¬¡ %d", currentCount, lastCount)
            
            // è¾“å‡ºåç¨‹æ ˆä¿¡æ¯
            buf := make([]byte, 1024*1024)
            stackSize := runtime.Stack(buf, true)
            log.Errorf("åç¨‹æ ˆä¿¡æ¯:\n%s", buf[:stackSize])
        }
        
        lastCount = currentCount
    }
}
```

### æ€§èƒ½è°ƒè¯•æŠ€å·§

```go
// æ€§èƒ½è°ƒè¯•å·¥å…·
type PerformanceProfiler struct {
    startTime time.Time
    samples   map[string][]time.Duration
    mu        sync.Mutex
}

func NewProfiler() *PerformanceProfiler {
    return &PerformanceProfiler{
        startTime: time.Now(),
        samples:   make(map[string][]time.Duration),
    }
}

func (pp *PerformanceProfiler) TimeFunction(name string, fn func()) {
    start := time.Now()
    fn()
    duration := time.Since(start)
    
    pp.mu.Lock()
    pp.samples[name] = append(pp.samples[name], duration)
    pp.mu.Unlock()
}

func (pp *PerformanceProfiler) GetReport() map[string]interface{} {
    pp.mu.Lock()
    defer pp.mu.Unlock()
    
    report := make(map[string]interface{})
    
    for name, durations := range pp.samples {
        if len(durations) == 0 {
            continue
        }
        
        var total time.Duration
        min := durations[0]
        max := durations[0]
        
        for _, d := range durations {
            total += d
            if d < min {
                min = d
            }
            if d > max {
                max = d
            }
        }
        
        avg := total / time.Duration(len(durations))
        
        report[name] = map[string]interface{}{
            "count":   len(durations),
            "total":   total.String(),
            "average": avg.String(),
            "min":     min.String(),
            "max":     max.String(),
        }
    }
    
    return report
}

// ä½¿ç”¨ç¤ºä¾‹
profiler := NewProfiler()

profiler.TimeFunction("message_send", func() {
    hub.SendToClient(clientID, message)
})

// å®šæœŸè¾“å‡ºæŠ¥å‘Š
go func() {
    for range time.Tick(60 * time.Second) {
        report := profiler.GetReport()
        log.Infof("æ€§èƒ½æŠ¥å‘Š: %+v", report)
    }
}()
```

### ç›‘æ§å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™
groups:
  - name: websocket_alerts
    rules:
      - alert: HighLatency
        expr: websocket_latency_p95 > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "WebSocket P95å»¶è¿Ÿè¿‡é«˜"
          description: "P95å»¶è¿Ÿ {{ $value }}ms è¶…è¿‡é˜ˆå€¼"
          
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 > 2048
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "å†…å­˜ä½¿ç”¨ {{ $value }}MB è¶…è¿‡2GB"
          
      - alert: TooManyConnections
        expr: websocket_active_connections > 50000
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "è¿æ¥æ•°è¿‡å¤š"
          description: "å½“å‰è¿æ¥æ•° {{ $value }} è¶…è¿‡å®‰å…¨é˜ˆå€¼"
```

---

é€šè¿‡ä»¥ä¸Šæ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œç›‘æ§æ–¹æ¡ˆï¼Œgo-wsc èƒ½å¤Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¨³å®šè¿è¡Œï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶å‘è¿æ¥å’Œé«˜ååé‡æ¶ˆæ¯ä¼ è¾“ã€‚å»ºè®®æ ¹æ®å®é™…ä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶æŒç»­ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚